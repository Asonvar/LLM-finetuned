# LLM-finetuned
AI^2 research lab task/project

i will be honest that i did use ChatGPT to my help. but i was not completely dependant on it.
i understood the logic and implementation behind the code and also modified the code myself thus removing the errors and bugs.
there are still some portions of llm that are difficult to me so some portion of my submission is unkown to me.
but i still take the responsibility of my submission.

so what is in my submission:



This project fine-tunes the 'DistilBERT' model on the SST-2 (Stanford Sentiment Treebank) dataset for sentiment classification.

The goal of this project is to fine-tune a small language model (DistilBERT) on a dataset with ~1,000 samples. This model is trained to classify sentences as either positive or negative.

## Files
- 'fine_tune_llm.ipynb': The notebook contains all the steps for fine-tuning the model.
- https://www.kaggle.com/code/adityasonvar/finetune-llm
- 'sst2_subset.csv' : the csv contains the subset of the dataset. it contains the reduced set to 1000 samples

IMPORTANT --> My previous submission used the entire dataset containing about 65000 samples so in the updated task i actually had to decrease it instead of increasing it.

## Dataset
- Dataset Used: [GLUE SST-2](https://huggingface.co/datasets/glue)


